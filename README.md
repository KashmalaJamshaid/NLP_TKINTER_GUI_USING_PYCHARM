# NATURAL LANGUAGE PROCESSING TOOL

The use of semantic web and a number of participatory platforms, such as web / text / opinion mining networks, online social networking, blogs, wikis, and forums, offer the potential for decision-makers in the public domain, government institutions, and civil society stakeholders to bring about major improvements in the way future communities operate. The changing technical landscape is transforming the transmission of information and the exchange of expertise between participants in public administration and even within civil society. The exponential development of the internet and the World Wide Web has resulted in a large volume of computer readable electronic information. This rising text data comes from different realms and has been steadily through for many centuries. Around 85 % of company knowledge is accessible in digital text format. According to Hearst, "Text Mining is the exploration by computer of new knowledge, previously unknown, by automatically collecting information from different written materials. A main aspect is to put the derived knowledge together in order to establish new evidence or new theories to be further investigated by more traditional means of experimentation. Text Mining is a modern area in computer science study and is primarily interdisciplinary, with close links to related areas such as data processing, artificial learning, information management, and computational linguistics. Text mining differs from data mining in a way that its fascinating trends are derived from structured data (databases) in data mining, whereas text mining processes semi-structured (XML files) or unstructured data (natural language text) and captures hidden meaningful data. Having these background information in mind we decided to create a project for natural language processing. Our project is based on three different phases namely:
1.	Text Extraction: The required input data for this part of text mining comes in XML file format. Relevant information is extracted automatically from these input files. This text is saved to separate text files after retrieving relevant data.
2.	Preprocessing / Text Cleansing: The next step is to clean the extracted text and this preprocessing module ensures that data is ready now for analysis process. Following preprocessing methods are involved in text cleaning.following preprocessing methods are involved in text cleaning.Following preprocessing methods are involved in text cleaning.


(i)	Tokenization
![Tokenization](https://user-images.githubusercontent.com/65674945/128155672-49b18c1e-53a2-4802-920b-17869fe64092.jpg)

(ii)	POS tagging
![POS_tagger](https://user-images.githubusercontent.com/65674945/128155623-cf70dfbd-2f0a-4723-8876-0c94007f4fd2.jpg)

(iii)	Stop Words Filtering
![Stopwords_Removal](https://user-images.githubusercontent.com/65674945/128155689-3b22579d-4377-4ef3-bbcf-529fdc9a3958.jpg)

(iv)	Lemmatization
![Lemmatization](https://user-images.githubusercontent.com/65674945/128155620-3f9ca6c3-e09b-4c22-92ef-59c0a931f14c.jpg)

(v)	TD/IDF 
![TDIDF](https://user-images.githubusercontent.com/65674945/128155666-b3b7cf29-9b05-45ec-9fbd-260cab488a98.jpg)

- Word Expansion and Matching: A list of the keywords is created at the end of the preprocessing step. This list of words originated from a file created by a model operation. Single file cannot provide sufficient information to generate the knowledge elements we need for the ontology specific to a domain. To enrich vocabulary with the necessary elements of understanding, we extract related terms (synonyms) from the WordNet dictionary created by Princeton University. 
![WordNet](https://user-images.githubusercontent.com/65674945/128155680-9f6ad8fb-1426-4484-b6f3-7274cbfcddaa.jpg)


